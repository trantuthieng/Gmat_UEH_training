import google.generativeai as genai
import json
import os
import random
from dotenv import load_dotenv
import time
from db import save_questions, get_cached_questions
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache

# Load environment variables
load_dotenv()

# --- C·∫§U H√åNH (Lazy init ƒë·ªÉ kh√¥ng g·ªçi Streamlit tr∆∞·ªõc set_page_config) ---

@lru_cache(maxsize=1)
def _get_api_key() -> str | None:
    """Fetch API key from env first; fallback to Streamlit secrets lazily.
    Avoid accessing streamlit at import time to keep set_page_config as first command.
    """
    key = os.getenv("GEMINI_API_KEY")
    if key:
        return key
    try:
        import streamlit as st  # imported lazily, after app has configured
        return st.secrets.get("GEMINI_API_KEY")
    except Exception:
        return None


@lru_cache(maxsize=1)
def _get_model():
    key = _get_api_key()
    if not key:
        print("GEMINI_API_KEY not found. Set in environment or Streamlit secrets.")
        return None
    try:
        genai.configure(api_key=key)
        return genai.GenerativeModel('gemma-3-12b-it')
    except Exception as e:
        print(f"L·ªói kh·ªüi t·∫°o Gemini: {e}")
        return None

def _clean_response_text(response) -> str:
    """Extracts the first text part and strips code fences/whitespace/control chars."""
    text = None
    try:
        text = response.text
    except Exception:
        pass

    if not text:
        try:
            text = response.candidates[0].content.parts[0].text  # best-effort fallback
        except Exception:
            text = ""

    # Remove markdown code fences
    text = text.replace('```json', '').replace('```', '').strip()
    
    # Remove control characters that break JSON (except \n, \r, \t)
    import re
    text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
    
    return text


def generate_question_variant(seed_question, max_attempts: int = 3):
    """T·∫°o 1 bi·∫øn th·ªÉ c√¢u h·ªèi (d√πng cho h√†m batch b√™n d∆∞·ªõi) v·ªõi retry khi JSON l·ªói."""
    model = _get_model()
    if model is None:
        print("‚ùå Model kh√¥ng ƒë∆∞·ª£c kh·ªüi t·∫°o")
        return None

    prompt = f"""
    ƒê√≥ng vai ng∆∞·ªùi ra ƒë·ªÅ thi GMAT.
    Ch·ªß ƒë·ªÅ: {seed_question.get('topic', 'Ki·∫øn th·ª©c t·ªïng h·ª£p')}
    C√¢u m·∫´u: "{seed_question['content']}"

    Nhi·ªám v·ª•: T·∫°o 1 c√¢u h·ªèi tr·∫Øc nghi·ªám M·ªöI:
    - N·∫øu l√† to√°n/logic: gi·ªØ nguy√™n d·∫°ng to√°n/logic nh∆∞ng thay s·ªë li·ªáu/b·ªëi c·∫£nh
    - N·∫øu l√† ki·∫øn th·ª©c: c√πng ch·ªß ƒë·ªÅ nh∆∞ng h·ªèi kh√≠a c·∫°nh kh√°c

    R√†ng bu·ªôc ƒë·ªãnh d·∫°ng:
    - Ch·ªâ d√πng k√Ω t·ª± ASCII, kh√¥ng k√Ω t·ª± ƒë·∫∑c bi·ªát, kh√¥ng emoji.
    - Kh√¥ng xu·ªëng d√≤ng trong gi√° tr·ªã chu·ªói.
    - Kh√¥ng d√πng Markdown, kh√¥ng bao c√°c block ```json.
    - Tr·∫£ v·ªÅ DUY NH·∫§T m·ªôt JSON object h·ª£p l·ªá.

    OUTPUT JSON duy nh·∫•t:
    {{
        "id": "new_id",
        "type": "general",
        "question": "No newline. Short and clear.",
        "options": ["A. ...", "B. ...", "C. ...", "D. ..."],
        "correct_answer": "Copy exact text of the correct option",
        "explanation": "Brief reasoning"
    }}
    """

    for attempt in range(1, max_attempts + 1):
        try:
            # Add generation config for better JSON output
            response = model.generate_content(
                prompt,
                generation_config={
                    'temperature': 0.7,
                    'max_output_tokens': 1024,
                    'response_mime_type': 'application/json'
                }
            )
            clean_text = _clean_response_text(response)
            data = json.loads(clean_text)
            data['type'] = 'general'  # T·∫•t c·∫£ ƒë·ªÅu l√† c√¢u h·ªèi chung
            return data
        except json.JSONDecodeError as e:
            print(f"‚ùå L·ªói JSON (attempt {attempt}/{max_attempts}): {e}")
            print(f"Response text: {clean_text[:200]}")
            if attempt < max_attempts:
                time.sleep(1 * attempt)  # Exponential backoff
        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫°o c√¢u (attempt {attempt}/{max_attempts}): {e}")
            if attempt < max_attempts:
                time.sleep(2 * attempt)  # Exponential backoff

    return None

def generate_question_batch(seeds, start_idx=0, progress_callback=None):
    """Generate multiple questions concurrently"""
    results = []
    visual_keywords = ['h√¨nh', 'shape', '·∫£nh', 'diagram', 'figure', 'bi·ªÉu ƒë·ªì']
    
    with ThreadPoolExecutor(max_workers=3) as executor:
        # Submit all tasks
        future_to_idx = {executor.submit(generate_question_variant, seed): (idx, seed) 
                        for idx, seed in enumerate(seeds)}
        
        # Process completed tasks
        for future in as_completed(future_to_idx):
            idx, seed = future_to_idx[future]
            try:
                new_q = future.result()
                if new_q:
                    text = (new_q.get('question') or '').lower()
                    has_image = bool(new_q.get('image_url'))
                    if any(k in text for k in visual_keywords) and not has_image:
                        print(f"üö´ B·ªè qua c√¢u h·ªèi thi·∫øu h√¨nh ·∫£nh: {text[:60]}...")
                    else:
                        results.append(new_q)
                        print(f"‚úÖ C√¢u {start_idx + idx + 1} - T·∫°o th√†nh c√¥ng")
                else:
                    print(f"‚ö†Ô∏è C√¢u {start_idx + idx + 1} - Th·∫•t b·∫°i")
            except Exception as e:
                print(f"‚ùå L·ªói khi t·∫°o c√¢u {start_idx + idx + 1}: {e}")
            
            if progress_callback:
                progress_callback((start_idx + idx + 1) / (start_idx + len(seeds)))
    
    return results

def generate_full_exam(seed_data, num_questions=30, num_general=0, progress_callback=None, max_retries_per_question=4):
    """
    T·∫°o b·ªô ƒë·ªÅ thi ho√†n ch·ªânh v·ªõi c∆° ch·∫ø concurrent execution v√† retry ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô.
    - num_questions: T·ªïng s·ªë c√¢u h·ªèi c·∫ßn t·∫°o
    - num_general: Tham s·ªë c≈© ƒë·ªÉ t∆∞∆°ng th√≠ch, b·ªè qua
    - max_retries_per_question: S·ªë l·∫ßn th·ª≠ l·∫°i t·ªëi ƒëa cho m·ªói c√¢u th·∫•t b·∫°i
    """
    exam_questions = []

    if not seed_data:
        print("‚ùå Kh√¥ng c√≥ seed data")
        return exam_questions

    print(f"üìã B·∫Øt ƒë·∫ßu t·∫°o {num_questions} c√¢u h·ªèi t·ª´ {len(seed_data)} c√¢u m·∫´u (concurrent mode)...")

    # Try to get from cache first
    cached = get_cached_questions(num_questions, randomize=True)
    if len(cached) >= num_questions:
        print(f"‚úÖ S·ª≠ d·ª•ng {num_questions} c√¢u t·ª´ cache")
        return cached[:num_questions]
    
    selected_seeds = random.choices(seed_data, k=num_questions)
    total_tasks = len(selected_seeds)

    # Concurrent generation - batch processing
    exam_questions = generate_question_batch(selected_seeds, 0, progress_callback)

    # Retry with concurrent processing for failed questions
    if len(exam_questions) < num_questions:
        remaining = num_questions - len(exam_questions)
        print(f"üîÅ B·∫Øt ƒë·∫ßu th·ª≠ l·∫°i cho {remaining} c√¢u l·ªói (concurrent retry)...")
        retry_pool = random.choices(seed_data, k=remaining * 2)  # Generate more to increase success rate
        
        retry_results = generate_question_batch(retry_pool, len(exam_questions), progress_callback)
        exam_questions.extend(retry_results[:remaining])

    # Ki·ªÉm tra c√¢u tr√πng l·∫∑p d·ª±a tr√™n n·ªôi dung c√¢u h·ªèi (optimized)
    seen_questions = set()
    unique_questions = []
    
    for q in exam_questions:
        question_text = q.get('question', '').strip().lower()
        if question_text and question_text not in seen_questions:
            unique_questions.append(q)
            seen_questions.add(question_text)
    
    exam_questions = unique_questions
    print(f"‚úÖ Lo·∫°i b·ªè tr√πng l·∫∑p: c√≤n {len(exam_questions)} c√¢u duy nh·∫•t")
    
    # Save to cache for future use
    if exam_questions:
        try:
            saved_count = save_questions(exam_questions)
            print(f"üíæ ƒê√£ l∆∞u {saved_count} c√¢u v√†o DB cache")
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ l∆∞u DB: {e}")
    
    # Fallback: use cache if still not enough
    if len(exam_questions) < num_questions:
        need_fill = num_questions - len(exam_questions)
        print(f"‚ö†Ô∏è C√≤n thi·∫øu {need_fill} c√¢u, s·ª≠ d·ª•ng cache ƒë·ªÉ b·ªï sung...")
        
        cached = get_cached_questions(need_fill * 2, randomize=True)
        for q in cached:
            q_text = q.get('question', '').strip().lower()
            if q_text not in seen_questions:
                exam_questions.append(q)
                seen_questions.add(q_text)
                if len(exam_questions) >= num_questions:
                    break

    # X√°o tr·ªôn th·ª© t·ª± c√¢u h·ªèi ƒë·ªÉ ƒë·∫£m b·∫£o ng·∫´u nhi√™n ho√†n to√†n
    random.shuffle(exam_questions)
    
    if len(exam_questions) < num_questions:
        print(f"‚ö†Ô∏è C·∫£nh b√°o: Ch·ªâ t·∫°o ƒë∆∞·ª£c {len(exam_questions)}/{num_questions} c√¢u. Vui l√≤ng ki·ªÉm tra API key ho·∫∑c th·ª≠ l·∫°i.")
    else:
        print(f"üéâ T·∫°o xong {len(exam_questions)} c√¢u h·ªèi (kh√¥ng tr√πng l·∫∑p, th·ª© t·ª± ng·∫´u nhi√™n)")
    
    return exam_questions